% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/vocab.R
\name{vocab}
\alias{vocab}
\alias{vocab}
\alias{vocab_update}
\alias{vocab}
\alias{vocab_prune}
\title{Build and manipulate vocabularies}
\usage{
vocab(corpus, ngram = c(1, 1), ngram_sep = "_", seps = " \\t\\n\\r")

vocab_update(vocab, corpus)

vocab_prune(vocab, max_terms = Inf, term_count_min = 1L,
  term_count_max = Inf, doc_proportion_min = 0, doc_proportion_max = 1,
  doc_count_min = 1L, doc_count_max = Inf, nbuckets = attr(vocab,
  "nbuckets"))
}
\arguments{
\item{corpus}{A collection of ASCII or UTF-8 encoded documents. It can be a
list of character vectors, a character vector or a data.frame with at
least two columns - id and documents. See details.}

\item{ngram}{a vector of length 2 of the form \code{c(min_ngram, max_ngram)} or a
singleton \code{max_ngram} which is equivalent to \code{c(1L, max_ngram)}.}

\item{ngram_sep}{separator to link terms within ngrams.}

\item{seps}{a scalar string containing characters to be used for document
splitting when \code{corpus} is a character vector; ignored otherwise. Defaults
to a set of basic white space separators. \code{NULL} means no
segmentation. Only ASCII and UTF-8 separators are supported. When UTF-8
characters separators are used, \code{corpus} is assumed to be in UTF-8
encoding (use \code{enc2utf8} to convert). Please note that in most cases more
sophisticated boundary segmentation algorithms (as implemented for example
in \code{\link[stringi:stri_split_boundaries]{stringi::stri_split_boundaries()}}) would do a better segmentation on
the expense of creating an intermediate list of segmented documents.}

\item{vocab}{\code{data.frame} obtained from a call to \code{\link[=vocab]{vocab()}}.}

\item{max_terms}{max number of terms to preserve}

\item{term_count_min}{keep terms occurring at \emph{least} this many times over
all docs}

\item{term_count_max}{keep terms occurring at \emph{most} this many times over
all docs}

\item{doc_count_min, doc_proportion_min}{keep terms appearing in at \emph{least}
this many docs}

\item{doc_count_max, doc_proportion_max}{keep terms appearing in at \emph{most}
this many docs}

\item{nbuckets}{How many unknown buckets to create along the remaining terms
of the pruned \code{vocab}. All pruned terms will be hashed into this many
buckets and the corresponding statistics (\code{term_count} and \code{doc_count})
updated.}
}
\description{
\code{\link[=vocab]{vocab()}} creates a vocabulary from a text corpus; \code{\link[=vocab_update]{vocab_update()}} and
\code{\link[=vocab_prune]{vocab_prune()}} update and prune an existing vocabulary respectively.
}
\details{
When \code{corpus} is a character vector each string is tokenized with \code{seps}
with the internal tokenizer. When \code{corpus} has names, names will be used to
name the output whenever appropriate.

When corpus is a \code{data.frame}, the documents must be in last column,  which
can be either a list of strings or a character vector. All other columns are
considered document ids. If first column is a character vector most function
will use it to name the output.
}
\examples{

corpus <-
   list(a = c("The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"), 
        b = c("the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog",
              "the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"))

vocab(corpus)
vocab(corpus, ngram = 3)
vocab(corpus, ngram = c(2, 3))

v <- vocab(corpus)

extra_corpus <- list(extras = c("apples", "oranges"))
v <- vocab_update(v, extra_corpus)
v

vocab_prune(v, max_terms = 7)
vocab_prune(v, term_count_min = 2)
vocab_prune(v, max_terms = 7, nbuckets = 2)

}
